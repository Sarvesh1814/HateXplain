{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f1ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a988fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380c60eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b318465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee3aea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>rationales</th>\n",
       "      <th>post_tokens</th>\n",
       "      <th>Target_cat</th>\n",
       "      <th>Label_cat</th>\n",
       "      <th>final_rationales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[]</td>\n",
       "      <td>['i', 'dont', 'think', 'im', 'getting', 'my', ...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[]</td>\n",
       "      <td>['we', 'cannot', 'continue', 'calling', 'ourse...</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[]</td>\n",
       "      <td>['nawt', 'yall', 'niggers', 'ignoring', 'me']</td>\n",
       "      <td>African</td>\n",
       "      <td>normal</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>['&lt;user&gt;', 'i', 'am', 'bit', 'confused', 'coz'...</td>\n",
       "      <td>Asian</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>['this', 'bitch', 'in', 'whataburger', 'eating...</td>\n",
       "      <td>Women</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>20143</td>\n",
       "      <td>9989999_gab</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...</td>\n",
       "      <td>['if', 'ur', 'still', 'on', 'twitter', 'tell',...</td>\n",
       "      <td>None</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20144</th>\n",
       "      <td>20144</td>\n",
       "      <td>9990225_gab</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n",
       "      <td>['when', 'i', 'first', 'got', 'on', 'here', 'a...</td>\n",
       "      <td>African</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20145</th>\n",
       "      <td>20145</td>\n",
       "      <td>9991681_gab</td>\n",
       "      <td>[]</td>\n",
       "      <td>['was', 'macht', 'der', 'moslem', 'wenn', 'der...</td>\n",
       "      <td>Other</td>\n",
       "      <td>normal</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20146</th>\n",
       "      <td>20146</td>\n",
       "      <td>9992513_gab</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...</td>\n",
       "      <td>['it', 'is', 'awful', 'look', 'at', 'world', '...</td>\n",
       "      <td>Asian</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20147</th>\n",
       "      <td>20147</td>\n",
       "      <td>9998729_gab</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...</td>\n",
       "      <td>['the', 'jewish', 'globalist', 'elite', 'have'...</td>\n",
       "      <td>African</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20148 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                      post_id  \\\n",
       "0               0  1179055004553900032_twitter   \n",
       "1               1  1179063826874032128_twitter   \n",
       "2               2  1178793830532956161_twitter   \n",
       "3               3  1179088797964763136_twitter   \n",
       "4               4  1179085312976445440_twitter   \n",
       "...           ...                          ...   \n",
       "20143       20143                  9989999_gab   \n",
       "20144       20144                  9990225_gab   \n",
       "20145       20145                  9991681_gab   \n",
       "20146       20146                  9992513_gab   \n",
       "20147       20147                  9998729_gab   \n",
       "\n",
       "                                              rationales  \\\n",
       "0                                                     []   \n",
       "1                                                     []   \n",
       "2                                                     []   \n",
       "3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4      [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "...                                                  ...   \n",
       "20143  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,...   \n",
       "20144  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n",
       "20145                                                 []   \n",
       "20146  [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,...   \n",
       "20147  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,...   \n",
       "\n",
       "                                             post_tokens Target_cat  \\\n",
       "0      ['i', 'dont', 'think', 'im', 'getting', 'my', ...       None   \n",
       "1      ['we', 'cannot', 'continue', 'calling', 'ourse...       None   \n",
       "2          ['nawt', 'yall', 'niggers', 'ignoring', 'me']    African   \n",
       "3      ['<user>', 'i', 'am', 'bit', 'confused', 'coz'...      Asian   \n",
       "4      ['this', 'bitch', 'in', 'whataburger', 'eating...      Women   \n",
       "...                                                  ...        ...   \n",
       "20143  ['if', 'ur', 'still', 'on', 'twitter', 'tell',...       None   \n",
       "20144  ['when', 'i', 'first', 'got', 'on', 'here', 'a...    African   \n",
       "20145  ['was', 'macht', 'der', 'moslem', 'wenn', 'der...      Other   \n",
       "20146  ['it', 'is', 'awful', 'look', 'at', 'world', '...      Asian   \n",
       "20147  ['the', 'jewish', 'globalist', 'elite', 'have'...    African   \n",
       "\n",
       "        Label_cat                                   final_rationales  \n",
       "0          normal  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1          normal  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2          normal                                    [0, 0, 0, 0, 0]  \n",
       "3      hatespeech  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      hatespeech  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...           ...                                                ...  \n",
       "20143   offensive  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...  \n",
       "20144   offensive  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...  \n",
       "20145      normal  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "20146  hatespeech  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, ...  \n",
       "20147   offensive  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[20148 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Downloads/HateXplain2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afcce698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweet_data, rationale_data, label_data, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tweet_data = tweet_data\n",
    "        self.rationale_data = rationale_data\n",
    "        self.label_data = label_data\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      tweet = str(self.tweet_data[idx])\n",
    "      rationale = self.rationale_data[idx]\n",
    "      label = self.label_data[idx]\n",
    "\n",
    "      # Tokenize the tweet\n",
    "      encoding = self.tokenizer.encode_plus(\n",
    "          tweet,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "      )\n",
    "\n",
    "      input_ids = encoding['input_ids'].squeeze()\n",
    "      attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "      # Convert the rationale to a tensor\n",
    "      rationale_tensor = torch.tensor(rationale)\n",
    "\n",
    "      return {\n",
    "          'input_ids': input_ids.long(),\n",
    "          'attention_mask': attention_mask.long(),\n",
    "          'rationale': rationale_tensor.long(),\n",
    "          'label': torch.tensor(label).long()\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6ab950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tweet_data = [' '.join(tweet) for tweet in df['post_tokens']]\n",
    "rationals = [eval(x) for x in df['final_rationales'].tolist()]\n",
    "label_dict = {'normal': 0, 'hatespeech': 1, 'offensive': 2}\n",
    "labels = [label_dict[x] for x in df['Label_cat'].tolist()]\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset = TweetDataset(tweet_data, rationals, labels, tokenizer, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d7d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    rationales = []\n",
    "    labels = []\n",
    "\n",
    "    for sample in batch:\n",
    "        input_ids.append(sample['input_ids'])\n",
    "        attention_masks.append(sample['attention_mask'])\n",
    "        rationales.append(sample['rationale'])\n",
    "        labels.append(sample['label'])\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True)\n",
    "    rationales = pad_sequence(rationales, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'rationale': rationales,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=46, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfc50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size) \n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=46, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=46, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bd86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████| 483/483 [00:00<00:00, 97.6kB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████| 268M/268M [00:37<00:00, 7.05MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable param3eters: 66955779\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch 1: Train Loss: 0.9809, Train Acc: 0.4773, Val Loss: 0.9460, Val Acc: 0.5089\n",
      "Epoch 2: Train Loss: 0.9246, Train Acc: 0.5252, Val Loss: 0.9287, Val Acc: 0.5208\n",
      "Epoch 3: Train Loss: 0.9082, Train Acc: 0.5367, Val Loss: 0.9169, Val Acc: 0.5342\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "num_params = model.num_parameters()\n",
    "print(f\"Number of trainable param3eters: {num_params}\")\n",
    "print(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train loop\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        rationale = batch['rationale']\n",
    "        labels = batch['label']\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute the accuracy\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        train_total += len(labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation loop\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        rationale = batch['rationale']\n",
    "        labels = batch['label']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            val_total += len(labels)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    # Print the results for this epoch\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    model.save_pretrained('Downloads/distilbert_model')\n",
    "    tokenizer.save_pretrained('Downloads/distilbert_tokenizer') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9c1cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"Downloads/distilbert_model\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"Downloads/distilbert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45273bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.8977, Train Acc: 0.5527, Val Loss: 0.8834, Val Acc: 0.5643\n",
      "Epoch 5: Train Loss: 0.8770, Train Acc: 0.5710, Val Loss: 0.8798, Val Acc: 0.5734\n",
      "Epoch 6: Train Loss: 0.8467, Train Acc: 0.5939, Val Loss: 0.8586, Val Acc: 0.5935\n",
      "Epoch 7: Train Loss: 0.8199, Train Acc: 0.6150, Val Loss: 0.8637, Val Acc: 0.6015\n",
      "Epoch 8: Train Loss: 0.7911, Train Acc: 0.6352, Val Loss: 0.8720, Val Acc: 0.5839\n",
      "Epoch 9: Train Loss: 0.7605, Train Acc: 0.6538, Val Loss: 0.9012, Val Acc: 0.5854\n",
      "Epoch 10: Train Loss: 0.7294, Train Acc: 0.6714, Val Loss: 0.9020, Val Acc: 0.5792\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 7\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train loop\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        rationale = batch['rationale']\n",
    "        labels = batch['label']\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute the accuracy\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        train_total += len(labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation loop\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        rationale = batch['rationale']\n",
    "        labels = batch['label']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            val_total += len(labels)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    # Print the results for this epoch\n",
    "    print(f\"Epoch {epoch+4}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    model.save_pretrained('Downloads/distilbert_model')\n",
    "    tokenizer.save_pretrained('Downloads/distilbert_tokenizer') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57598be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "  model.save_pretrained('/content/drive/MyDrive/AML/distilbert_model')\n",
    "  tokenizer.save_pretrained('/content/drive/MyDrive/AML/distilbert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe8d13ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"I hate bloody pakistanis\"\n",
    "\n",
    "input_ids = tokenizer.encode_plus(input_text,\n",
    "                                   add_special_tokens=True,\n",
    "                                   max_length=128,\n",
    "                                   return_token_type_ids=False,\n",
    "                                   return_attention_mask=True,\n",
    "                                   return_tensors='pt')['input_ids']\n",
    "\n",
    "# make the prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids)[0]\n",
    "    probabilities = torch.softmax(output, dim=1)[0]\n",
    "    \n",
    "# get the predicted label\n",
    "predicted_label = torch.argmax(probabilities).item()\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eebe009e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normal': 0, 'hatespeech': 1, 'offensive': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
